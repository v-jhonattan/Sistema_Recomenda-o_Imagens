{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v-jhonattan/Sistema_Recomenda-o_Imagens/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fPkbV-RJxQ2"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade pip\n",
        "# PyTorch CPU\n",
        "!pip -q install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
        "# Embeddings CLIP + índice Annoy + utilitários\n",
        "!pip -q install open_clip_torch annoy pillow matplotlib numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSscHSq7KhDM"
      },
      "outputs": [],
      "source": [
        "import os, glob, math, time, json, random, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import open_clip\n",
        "from annoy import AnnoyIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bk2ph3EjL2XI"
      },
      "outputs": [],
      "source": [
        "# Instalar libs\n",
        "!pip -q install icrawler imagehash pillow tqdm\n",
        "\n",
        "# Criar pastas\n",
        "!mkdir -p /content/data/{watches,tshirts,bikes,sneakers}\n",
        "\n",
        "# Baixar imagens por classe (BingImageCrawler)\n",
        "from icrawler.builtin import BingImageCrawler\n",
        "from pathlib import Path\n",
        "\n",
        "CLASSES = {\n",
        "    \"watches\":  [\"wristwatch\", \"analog watch\", \"round dial watch\"],\n",
        "    \"tshirts\":  [\"t shirt front\", \"plain tshirt\", \"graphic tshirt\"],\n",
        "    \"bikes\":    [\"bicycle\", \"mountain bike\", \"road bike\"],\n",
        "    \"sneakers\": [\"sneakers\", \"running shoes\", \"tennis shoes\"]\n",
        "}\n",
        "TARGET_PER_CLASS = 400\n",
        "PER_QUERY_LIMIT   = 200\n",
        "\n",
        "def download_class(root, queries):\n",
        "    total = 0\n",
        "    for q in queries:\n",
        "        crawler = BingImageCrawler(storage={'root_dir': root})\n",
        "        crawler.crawl(keyword=q,\n",
        "                      max_num=PER_QUERY_LIMIT,\n",
        "                      filters={'type':'photo','size':'large'})\n",
        "\n",
        "        total = len(list(Path(root).rglob('*')))\n",
        "        if total >= TARGET_PER_CLASS: break\n",
        "    print(f\"[OK] baixadas ~{total} imagens em {root}\")\n",
        "\n",
        "for cls, queries in CLASSES.items():\n",
        "    download_class(f\"/content/data/{cls}\", queries)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dgZ9QMHOOXf"
      },
      "outputs": [],
      "source": [
        "!pip -q install icrawler imagehash pillow tqdm\n",
        "!mkdir -p /content/data/{watches,tshirts,bikes,sneakers}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOmbN56kP1jr"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, UnidentifiedImageError\n",
        "import imagehash, shutil, os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def clean_folder(folder, min_side=128):\n",
        "    raw = Path(folder)\n",
        "    tmp = raw.parent / (raw.name + \"_clean\")\n",
        "    tmp.mkdir(exist_ok=True)\n",
        "    seen = set(); kept = 0\n",
        "    for p in tqdm(list(raw.rglob(\"*\"))):\n",
        "        if not p.is_file():\n",
        "            continue\n",
        "        try:\n",
        "            im = Image.open(p).convert(\"RGB\")\n",
        "            if min(im.size) < min_side:\n",
        "                continue\n",
        "            h = imagehash.phash(im)\n",
        "            if h in seen:\n",
        "                continue\n",
        "            seen.add(h)\n",
        "            out = tmp / f\"{kept:06d}.jpg\"\n",
        "            im.save(out, \"JPEG\", quality=90, optimize=True)\n",
        "            kept += 1\n",
        "        except (UnidentifiedImageError, OSError):\n",
        "            continue\n",
        "    shutil.rmtree(raw)\n",
        "    tmp.rename(raw)\n",
        "    print(f\"[CLEAN] {raw} -> {kept} imagens úteis\")\n",
        "\n",
        "for cls in CLASSES:\n",
        "    clean_folder(f\"/content/data/{cls}\", min_side=128)\n",
        "\n",
        "# relatório final\n",
        "import os\n",
        "for cls in CLASSES:\n",
        "    n = sum(len(files) for _,_,files in os.walk(f\"/content/data/{cls}\"))\n",
        "    print(f\"{cls}: {n} imagens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuxn07vqSrvW"
      },
      "outputs": [],
      "source": [
        "!pip -q install icrawler imagehash pillow tqdm\n",
        "!mkdir -p /content/data/{watches,tshirts,bikes,sneakers}\n",
        "\n",
        "from icrawler.builtin import BingImageCrawler, GoogleImageCrawler\n",
        "from pathlib import Path\n",
        "from time import sleep\n",
        "\n",
        "CLASSES = {\n",
        "    \"watches\":  [\"wristwatch\", \"analog watch\", \"round dial watch\",\n",
        "                 \"diver watch\", \"dress watch\", \"digital watch\", \"smartwatch metal band\"],\n",
        "    \"tshirts\":  [\"plain t shirt front\", \"tshirt back view\", \"crew neck t-shirt blank\",\n",
        "                 \"white tshirt front\", \"black tshirt front\", \"graphic tshirt\"],\n",
        "    \"bikes\":    [\"road bicycle side view\", \"mountain bike hardtail\", \"city bike side view\",\n",
        "                 \"bmx bicycle side\", \"gravel bike\", \"folding bicycle\"],\n",
        "    \"sneakers\": [\"running shoe side view\", \"sneakers flat lay\", \"tennis shoes\",\n",
        "                 \"basketball sneakers\", \"trail running shoes\"]\n",
        "}\n",
        "TARGET_PER_CLASS = 300\n",
        "PER_QUERY_LIMIT  = 160\n",
        "\n",
        "def count_files(folder): return len([p for p in Path(folder).glob(\"**/*\") if p.is_file()])\n",
        "\n",
        "def crawl(engine, root, keyword, max_num, large=True):\n",
        "    if engine == \"bing\":\n",
        "        crawler = BingImageCrawler(storage={'root_dir': root}, downloader_threads=8, parser_threads=4)\n",
        "        filters = {'type':'photo','size':'large' if large else 'medium'}\n",
        "        crawler.crawl(keyword=keyword, max_num=max_num, filters=filters)\n",
        "    else:\n",
        "        crawler = GoogleImageCrawler(storage={'root_dir': root}, downloader_threads=8, parser_threads=4)\n",
        "        crawler.crawl(keyword=keyword, max_num=max_num)\n",
        "\n",
        "def topup_class(cls, queries):\n",
        "    root = f\"/content/data/{cls}\"\n",
        "    before = count_files(root)\n",
        "    print(f\"\\n==> {cls}: {before} arquivos antes\")\n",
        "    for q in queries:\n",
        "        try:  crawl(\"bing\", root, q, PER_QUERY_LIMIT, large=True)\n",
        "        except Exception as e: print(\"[bing/large]\", q, e)\n",
        "        if count_files(root) >= TARGET_PER_CLASS: break\n",
        "        try:  crawl(\"bing\", root, q, PER_QUERY_LIMIT//2, large=False)  # medium\n",
        "        except Exception as e: print(\"[bing/medium]\", q, e)\n",
        "        if count_files(root) >= TARGET_PER_CLASS: break\n",
        "        try:  crawl(\"google\", root, q, PER_QUERY_LIMIT//2)\n",
        "        except Exception as e: print(\"[google]\", q, e)\n",
        "        if count_files(root) >= TARGET_PER_CLASS: break\n",
        "        sleep(0.8)\n",
        "    after = count_files(root)\n",
        "    print(f\"[OK] {cls}: {after} arquivos depois\")\n",
        "\n",
        "for cls, qs in CLASSES.items():\n",
        "    topup_class(cls, qs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zci5B3pMUwoc"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, UnidentifiedImageError\n",
        "import imagehash, shutil, os, glob\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "def clean_folder(folder, min_side=128):\n",
        "    raw = Path(folder)\n",
        "    tmp = raw.parent / (raw.name + \"_clean\")\n",
        "    tmp.mkdir(exist_ok=True)\n",
        "    seen = set(); kept = 0\n",
        "    for p in tqdm(list(raw.rglob(\"*\"))):\n",
        "        if not p.is_file(): continue\n",
        "        try:\n",
        "            im = Image.open(p).convert(\"RGB\")\n",
        "            if min(im.size) < min_side:\n",
        "                continue\n",
        "            h = imagehash.phash(im)\n",
        "            if h in seen:\n",
        "                continue\n",
        "            seen.add(h)\n",
        "            out = tmp / f\"{kept:06d}.jpg\"\n",
        "            im.save(out, \"JPEG\", quality=90, optimize=True)\n",
        "            kept += 1\n",
        "        except (UnidentifiedImageError, OSError):\n",
        "            continue\n",
        "    shutil.rmtree(raw)\n",
        "    tmp.rename(raw)\n",
        "    print(f\"[CLEAN] {raw} -> {kept} imagens úteis\")\n",
        "\n",
        "for cls in CLASSES:\n",
        "    clean_folder(f\"/content/data/{cls}\", min_side=128)\n",
        "\n",
        "# relatório\n",
        "import os\n",
        "for cls in CLASSES:\n",
        "    n = sum(len(files) for _,_,files in os.walk(f\"/content/data/{cls}\"))\n",
        "    print(f\"{cls}: {n} imagens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGkXB8fxVPKs"
      },
      "outputs": [],
      "source": [
        "# instala o downloader de Open Images\n",
        "!pip -q install openimages\n",
        "import os, glob, shutil\n",
        "\n",
        "LABEL_MAP = {\n",
        "    \"watches\":  \"Wristwatch\",\n",
        "    \"tshirts\":  \"T-shirt\",\n",
        "    \"bikes\":    \"Bicycle\",\n",
        "    \"sneakers\": \"Sneakers\",\n",
        "}\n",
        "\n",
        "def topup_openimages(cls, target=TARGET_PER_CLASS):\n",
        "    dst = f\"/content/data/{cls}\"\n",
        "    cur = len(glob.glob(f\"{dst}/*.jpg\"))\n",
        "    need = target - cur\n",
        "    if need <= 0:\n",
        "        print(f\"[OI] {cls} já tem {cur}.\"); return\n",
        "    label = LABEL_MAP[cls]\n",
        "    tmp = f\"/content/oi_tmp_{cls}\"\n",
        "    os.makedirs(tmp, exist_ok=True)\n",
        "    print(f\"[OI] baixando {need} para {cls} ({label})…\")\n",
        "    # baixa em tmp (pode trazer menos que o pedido)\n",
        "    !openimages download --classes \"{label}\" --limit {need} --format jpg --dest {tmp} >/dev/null\n",
        "    # move e limpa\n",
        "    moved = 0\n",
        "    for p in glob.glob(f\"{tmp}/{label}/*.jpg\"):\n",
        "        shutil.move(p, f\"{dst}/\")\n",
        "        moved += 1\n",
        "    shutil.rmtree(tmp, ignore_errors=True)\n",
        "    print(f\"[OI] movidas {moved} imagens para {dst}\")\n",
        "\n",
        "# completar por classe\n",
        "for cls in CLASSES:\n",
        "    topup_openimages(cls, target=TARGET_PER_CLASS)\n",
        "\n",
        "# limpeza final pós-merge\n",
        "for cls in CLASSES:\n",
        "    clean_folder(f\"/content/data/{cls}\", min_side=128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eavP6veKVxl-"
      },
      "outputs": [],
      "source": [
        "import os, glob, math, time, json, random, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import open_clip\n",
        "from annoy import AnnoyIndex\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "# Carrega CLIP (visual + text) – 512D de saída no ViT-B/32\n",
        "model_name, pretrained = \"ViT-B-32\", \"laion2b_s34b_b79k\"\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained, device=device)\n",
        "tokenizer = open_clip.get_tokenizer(model_name)\n",
        "model.eval()\n",
        "\n",
        "# Transform simples p/ manter consistência\n",
        "to_tensor = preprocess\n",
        "\n",
        "@torch.no_grad()\n",
        "def embed_image(path_or_pil):\n",
        "    if isinstance(path_or_pil, str):\n",
        "        img = Image.open(path_or_pil).convert(\"RGB\")\n",
        "    else:\n",
        "        img = path_or_pil.convert(\"RGB\")\n",
        "    x = to_tensor(img).unsqueeze(0).to(device)\n",
        "    feat = model.encode_image(x)\n",
        "    feat = feat / feat.norm(dim=-1, keepdim=True)  # normaliza (unit norm)\n",
        "    return feat.squeeze(0).cpu().numpy()  # (512,)\n",
        "\n",
        "@torch.no_grad()\n",
        "def embed_text(text: str):\n",
        "    tokens = tokenizer([text]).to(device)\n",
        "    feat = model.encode_text(tokens)\n",
        "    feat = feat / feat.norm(dim=-1, keepdim=True)\n",
        "    return feat.squeeze(0).cpu().numpy()\n",
        "\n",
        "def show_grid(paths, titles=None, cols=4, size=3):\n",
        "    rows = math.ceil(len(paths)/cols)\n",
        "    plt.figure(figsize=(cols*size, rows*size))\n",
        "    for i,p in enumerate(paths):\n",
        "        ax = plt.subplot(rows, cols, i+1)\n",
        "        img = Image.open(p).convert(\"RGB\")\n",
        "        plt.imshow(img)\n",
        "        ax.set_axis_off()\n",
        "        if titles and i < len(titles):\n",
        "            ax.set_title(titles[i], fontsize=9)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29UMrbkPWq2O"
      },
      "outputs": [],
      "source": [
        "\n",
        "DATA_DIR = \"/content/data\"  # ajuste se necessário\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA8biRnrWwMv"
      },
      "outputs": [],
      "source": [
        "def list_images(root, exts=(\"jpg\",\"jpeg\",\"png\",\"webp\")):\n",
        "    paths = []\n",
        "    for ext in exts:\n",
        "        paths += glob.glob(os.path.join(root, \"**\", f\"*.{ext}\"), recursive=True)\n",
        "    return sorted(paths)\n",
        "\n",
        "paths = list_images(DATA_DIR)\n",
        "print(\"N imagens:\", len(paths))\n",
        "print(\"\\nAlguns exemplos:\")\n",
        "for p in paths[:5]:\n",
        "    print(\" -\", p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZdX-af_W7M9"
      },
      "outputs": [],
      "source": [
        "EMB_DIM = 512               # ViT-B/32 = 512\n",
        "N_TREES = 50\n",
        "INDEX_PATH = \"/content/image_index.ann\"\n",
        "META_PATH  = \"/content/image_meta.json\"\n",
        "EMB_NPZ    = \"/content/image_embs.npz\"\n",
        "\n",
        "def build_index(image_paths):\n",
        "    index = AnnoyIndex(EMB_DIM, metric='angular')\n",
        "    meta = []\n",
        "    embs = np.zeros((len(image_paths), EMB_DIM), dtype=np.float32)\n",
        "\n",
        "    t0 = time.time()\n",
        "    for i, p in enumerate(image_paths):\n",
        "        try:\n",
        "            v = embed_image(p).astype(np.float32)\n",
        "            index.add_item(i, v)\n",
        "            embs[i] = v\n",
        "            meta.append({\"id\": i, \"path\": p})\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] falhou {p}: {e}\")\n",
        "    index.build(N_TREES)\n",
        "    index.save(INDEX_PATH)\n",
        "    with open(META_PATH, \"w\") as f:\n",
        "        json.dump(meta, f)\n",
        "    np.savez(EMB_NPZ, embs=embs)\n",
        "    print(f\"Index criado em {time.time()-t0:.1f}s | itens indexados: {len(meta)}\")\n",
        "    return index, meta, embs\n",
        "\n",
        "# Constrói (ou reusa, se já existir)\n",
        "if os.path.exists(INDEX_PATH) and os.path.exists(META_PATH) and os.path.exists(EMB_NPZ):\n",
        "    idx = AnnoyIndex(EMB_DIM, metric='angular'); idx.load(INDEX_PATH)\n",
        "    meta = json.load(open(META_PATH))\n",
        "    embs = np.load(EMB_NPZ)[\"embs\"]\n",
        "    print(\"Index carregado.\")\n",
        "else:\n",
        "    idx, meta, embs = build_index(paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gn93pHFYIbZ"
      },
      "outputs": [],
      "source": [
        "def search_by_image(query_path, k=12):\n",
        "    q = embed_image(query_path).astype(np.float32)\n",
        "    ids, dists = idx.get_nns_by_vector(q, k, include_distances=True)\n",
        "    recs = [meta[i][\"path\"] for i in ids]\n",
        "    scores = [1 - d for d in dists]  # angular -> ~similaridade\n",
        "    return recs, scores\n",
        "\n",
        "# Pqegar qualquer imagem do dataset como consulta\n",
        "query_img = paths[0] if paths else None\n",
        "if query_img:\n",
        "    recs, scores = search_by_image(query_img, k=8)\n",
        "    print(\"Consulta:\", query_img)\n",
        "    show_grid([query_img] + recs, titles=[\"QUERY\"] + [f\"{s:.2f}\" for s in scores], cols=4, size=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXwgXc9vYOyw"
      },
      "outputs": [],
      "source": [
        "def search_by_text(text, k=12):\n",
        "    q = embed_text(text).astype(np.float32)\n",
        "    ids, dists = idx.get_nns_by_vector(q, k, include_distances=True)\n",
        "    recs = [meta[i][\"path\"] for i in ids]\n",
        "    scores = [1 - d for d in dists]\n",
        "    return recs, scores\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wuPexI9Ydif"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def scan_labeled(root):\n",
        "    items, labels = [], []\n",
        "    classes = sorted([d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))])\n",
        "    cls2id = {c:i for i,c in enumerate(classes)}\n",
        "    for c in classes:\n",
        "        for p in list_images(os.path.join(root, c)):\n",
        "            items.append(p); labels.append(cls2id[c])\n",
        "    return items, labels, classes\n",
        "\n",
        "LABELED_DIR = DATA_DIR\n",
        "train_paths, train_labels, classes = scan_labeled(LABELED_DIR)\n",
        "print(f\"{len(train_paths)} imagens rotuladas | classes: {classes}\")\n",
        "\n",
        "class EmbedDataset(Dataset):\n",
        "    def __init__(self, paths, labels):\n",
        "        self.paths = paths; self.labels = labels\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        x = embed_image(self.paths[idx]).astype(np.float32)  # (512,)\n",
        "        y = self.labels[idx]\n",
        "        return torch.from_numpy(x), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "ds = EmbedDataset(train_paths, train_labels)\n",
        "dl = DataLoader(ds, batch_size=64, shuffle=True)\n",
        "\n",
        "# Cabeça linear (512 -> num_classes)\n",
        "num_classes = len(classes)\n",
        "clf = nn.Linear(EMB_DIM, num_classes)\n",
        "clf.to(device)\n",
        "opt = optim.AdamW(clf.parameters(), lr=1e-3)\n",
        "crit = nn.CrossEntropyLoss()\n",
        "\n",
        "# Treino rápido\n",
        "EPOCHS = 10\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    clf.train()\n",
        "    losses = []\n",
        "    for xb, yb in dl:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = clf(xb)\n",
        "        loss = crit(logits, yb)\n",
        "        loss.backward(); opt.step()\n",
        "        losses.append(loss.item())\n",
        "    print(f\"época {ep} | loss {np.mean(losses):.4f}\")\n",
        "\n",
        "# Inferência de classe\n",
        "@torch.no_grad()\n",
        "def predict_class(image_path):\n",
        "    x = torch.from_numpy(embed_image(image_path).astype(np.float32)).unsqueeze(0).to(device)\n",
        "    logits = clf(x); prob = logits.softmax(dim=-1)[0]\n",
        "    pid = int(torch.argmax(prob).cpu())\n",
        "    return classes[pid], float(prob[pid].cpu())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR69uhBBJevM"
      },
      "outputs": [],
      "source": [
        "import random, numpy as np, torch\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# 1) split estratificado (80/20)\n",
        "paths = np.array(train_paths)\n",
        "labels = np.array(train_labels)\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, val_idx = next(sss.split(paths, labels))\n",
        "val_paths, val_labels = paths[val_idx], labels[val_idx]\n",
        "\n",
        "# 2) dataset/dataloader de validação\n",
        "class EmbedDatasetEval(Dataset):\n",
        "    def __init__(self, paths, labels): self.paths, self.labels = list(paths), list(labels)\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, i):\n",
        "        x = embed_image(self.paths[i]).astype(np.float32)\n",
        "        y = self.labels[i]\n",
        "        return torch.from_numpy(x), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "val_ds = EmbedDatasetEval(val_paths, val_labels)\n",
        "val_dl = DataLoader(val_ds, batch_size=128, shuffle=False)\n",
        "\n",
        "# 3) métrica\n",
        "@torch.no_grad()\n",
        "def eval_acc(dataloader):\n",
        "    clf.eval()\n",
        "    correct, total = 0, 0\n",
        "    per_cls = {i:[0,0] for i in range(len(classes))}\n",
        "    for xb, yb in dataloader:\n",
        "        logits = clf(xb.to(device))\n",
        "        pred = torch.argmax(logits, dim=-1).cpu()\n",
        "        y = yb\n",
        "        correct += (pred==y).sum().item()\n",
        "        total += y.size(0)\n",
        "        for yi, pi in zip(y.tolist(), pred.tolist()):\n",
        "            per_cls[yi][1]+=1\n",
        "            if yi==pi: per_cls[yi][0]+=1\n",
        "    acc = correct/total\n",
        "    per_cls_acc = {classes[i]: c/t if t else 0 for i,(c,t) in per_cls.items()}\n",
        "    return acc, per_cls_acc\n",
        "\n",
        "acc, per_cls_acc = eval_acc(val_dl)\n",
        "print(f\"ACC validação: {acc:.3f}\")\n",
        "print(\"ACC por classe:\", per_cls_acc)\n",
        "\n",
        "\n",
        "torch.save({\"state_dict\": clf.state_dict(), \"classes\": classes}, \"/content/cls_head.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvYzMd3tJ4SF"
      },
      "outputs": [],
      "source": [
        "import os, json, time, numpy as np\n",
        "from annoy import AnnoyIndex\n",
        "\n",
        "EMB_DIM = 512\n",
        "INDEX_PATH = \"/content/image_index.ann\"\n",
        "META_PATH  = \"/content/image_meta.json\"\n",
        "EMB_NPZ    = \"/content/image_embs.npz\"\n",
        "\n",
        "# coletores (caminho -> classe opcional)\n",
        "cat_paths = train_paths  # já tem TODAS as imagens\n",
        "cat_labels = train_labels\n",
        "\n",
        "# extrair e salvar embeddings (1x)\n",
        "def compute_all_embeddings(paths):\n",
        "    embs = np.zeros((len(paths), EMB_DIM), dtype=np.float32)\n",
        "    t0 = time.time()\n",
        "    for i,p in enumerate(paths):\n",
        "        embs[i] = embed_image(p).astype(np.float32)\n",
        "        if (i+1)%200==0: print(f\"{i+1}/{len(paths)}\")\n",
        "    print(f\"[OK] embeddings em {time.time()-t0:.1f}s\")\n",
        "    return embs\n",
        "\n",
        "embs = compute_all_embeddings(cat_paths)\n",
        "np.savez(EMB_NPZ, embs=embs)\n",
        "\n",
        "# criar índice Annoy (métrica angular ~ cosseno)\n",
        "idx = AnnoyIndex(EMB_DIM, metric='angular')\n",
        "for i in range(len(cat_paths)):\n",
        "    idx.add_item(i, embs[i])\n",
        "idx.build(50)\n",
        "idx.save(INDEX_PATH)\n",
        "\n",
        "# salvar metadados (caminho + classe)\n",
        "meta = [{\"id\": i, \"path\": cat_paths[i], \"label\": int(cat_labels[i])} for i in range(len(cat_paths))]\n",
        "json.dump(meta, open(META_PATH, \"w\"))\n",
        "print(\"Índice e metadados salvos.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4urYuog4K8ld"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# carregar índice/metadados\n",
        "idx = AnnoyIndex(EMB_DIM, metric='angular'); idx.load(INDEX_PATH)\n",
        "meta = json.load(open(META_PATH))\n",
        "embs = np.load(EMB_NPZ)[\"embs\"]\n",
        "\n",
        "def search_by_image(query_path, k=12, exclude_identical=True):\n",
        "    q = embed_image(query_path).astype(np.float32)\n",
        "    ids, dists = idx.get_nns_by_vector(q, k + (1 if exclude_identical else 0), include_distances=True)\n",
        "\n",
        "    if exclude_identical:\n",
        "        ids = [i for i in ids if meta[i][\"path\"] != query_path][:k]\n",
        "        dists = dists[:len(ids)]\n",
        "    recs = [meta[i][\"path\"] for i in ids]\n",
        "    scores = [1 - d for d in dists]\n",
        "    return ids, recs, scores\n",
        "\n",
        "def show_grid(paths, titles=None, cols=4, size=2.6):\n",
        "    import math\n",
        "    rows = math.ceil(len(paths)/cols)\n",
        "    plt.figure(figsize=(cols*size, rows*size))\n",
        "    for i,p in enumerate(paths):\n",
        "        ax = plt.subplot(rows, cols, i+1)\n",
        "        plt.imshow(Image.open(p).convert(\"RGB\"))\n",
        "        ax.set_axis_off()\n",
        "        if titles and i < len(titles): ax.set_title(titles[i], fontsize=8)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# exemplo de consulta\n",
        "q = random.choice(train_paths)\n",
        "ids, recs, scores = search_by_image(q, k=12)\n",
        "show_grid([q] + recs, titles=[\"QUERY\"] + [f\"{s:.2f}\" for s in scores])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-4ziDznLMru"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def precision_at_k(k=10, sample=300):\n",
        "    rng = np.random.default_rng(42)\n",
        "    idxs = rng.choice(len(cat_paths), size=min(sample, len(cat_paths)), replace=False)\n",
        "    hits, total = 0, 0\n",
        "    per_cls = defaultdict(lambda: [0,0])  # acertos, consultas\n",
        "    for i in idxs:\n",
        "        q_path, q_label = cat_paths[i], cat_labels[i]\n",
        "        ids, _, _ = search_by_image(q_path, k=k, exclude_identical=True)\n",
        "        retrieved_labels = [meta[j][\"label\"] for j in ids]\n",
        "        h = sum(1 for r in retrieved_labels if r == int(q_label))\n",
        "        hits += h; total += k\n",
        "        per_cls[classes[q_label]][0] += h\n",
        "        per_cls[classes[q_label]][1] += k\n",
        "    Pk_global = hits/total\n",
        "    Pk_cls = {c: (a/b if b else 0) for c,(a,b) in per_cls.items()}\n",
        "    return Pk_global, Pk_cls\n",
        "\n",
        "Pk, Pk_cls = precision_at_k(k=10, sample=400)\n",
        "print(f\"Precision@10 (global): {Pk:.3f}\")\n",
        "print(\"Precision@10 por classe:\", Pk_cls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60YxTibtL7LD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Classificador (head)\n",
        "torch.save({\"state_dict\": clf.state_dict(), \"classes\": classes}, \"/content/cls_head.pt\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}